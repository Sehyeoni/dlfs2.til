# Chapter8 Attention
## 8.1 어텐션의 구조
[어텐션 메커니즘] 인간처럼 필요한 정보에만 주목할 수 있게 한다.

### 8.1.1 seq2seq의 문제점
인코더는 고정길이의 벡터를 출력하는데, 이 '고정길이'에 문제가 있다.<br/>
아무리 긴 문장이라도 항상 똑같은 길이의 벡터에 밀어넣게 되는것인데, 필요한 정보를 벡터에 다 담지 못하게 된다.

### 8.1.2 encoder 개선
출력의 길이를 입력 문장의 길이에 따라 바꿔주자.<br/>
시각별 LSTM 계층의 은닉 상태 벡터를 모두 이용하여(hs로 표기) 입력된 단어와 같은 수의 벡터를 얻는다.

많은 딥러닝 프레임워크에서 계층을 초기화할때 '모든 시각의 은닉 상태 벡터 반환'과 '마지막 은닉 상태 벡터만 반환'중 선택할 수 있다.

![인코더개선](fig%208-2.png)
![인코더의 출력 hs](fig%208-3.png)

hs행렬은 각 단어에 해당하는 벡터들의 집합이라고 볼 수 있다.<br/>
(특정 단어의 주변 정보를 균형있게 담아야 하는 경우, 시계열 데이터를 양방향으로 처리하는 양방향 RNN이나 양방향 LSTM이 효과적이다.)

### 8.1.3, 8.1.4, 8.1.5 decoder 개선
인코더는 각 단어에 대응하는 LSTM 계층의 은닉 상내 벡터를 hs로 모아 출력하고, hs가 디코더에 전달되어 시계열 변환이 이뤄진다.

디코더도 hs의 마지막 줄만이 아니라, 전부를 활용할 수 있도록 개선한다.

* 목표 : '도착어 단어'와 대응 관계에 있는 '출발어 단어'의 정보를 골라내고, 번역을 수행한다.<br/>즉, 필요한 정보에만 주목하여 그 정보로부터 시계열 변환을 수행한다. = 어텐션

![어텐션](fig%208-6.png)

위 그림에서 '어떤 계산'을 통해 대응어 선택 작업을 해내겠다. 문제점은 이 선택작업은 미분이 안된다.(=오차역전파법이 안된다.)

미분 가능하게 하려면 '하나를 선택'하지 않고 '모든 것을 선택'하게 해주고, 가중치는 별도로 계산하도록 한다.
(Weight Sum 가중)

각 단어의 중요도를 나타내는 가중치로, 가중합을 이용해 맥락 벡터를 얻을 수 있다.<br/>
가중치를 구하기 위해서는 벡터의 내적을 해서 유사도를 계산한다.
(Attention weight)

Attention 계층
![Attention 계층](fig%208-17.png)

![개선된 디코더](fig%208-18.png)

## 8.2 어텐션 구현, 8.3 어텐션 평가
attention.py

## 8.4 어텐션에 관한 남은 이야기
### 8.4.1 양방향 RNN
특정 단어의 주변 정보까지 균형있게 담고 싶은 경우, LSTM을 양방향으로 처리하는 방법을 생각해볼 수 있다.

데이터를 왼->오 / 오->왼 거꾸로 입력하여 처리하고, 이 두 계층의 출력을 연결해주면 된다.

### 8.4.2 Attention 계층 사용 방법
책에서는 Attetion 계층을 LSTM계층과 Affine 계층사이에 삽입했는데, 꼭 그럴 필요는 없다.

계층의 위치를 다르게 하는게 최종 정확도에 미치는 영향은 해봐야 알 수 있다.

### 8.4.3 seq2seq 심층화와 skip 연결
복잡한 문제를 풀어내기 위한 방안 : 층을 깊게 쌓는 방법을 활용하자.

skip 연결(잔차 연결 residual connection, 숏컷 short-cut) : 계층을 ㄷ건너뛰는 연결
![skip 연결의 예](fig%208-34.png)

skip 연결의 접속부에 2개의 출력이 더해지는데, 덧셈 과정에서는 역전파시 기울기가 그대로 흘러가므로 기울시 소실이나 폭발 문제가 일어나지 않는다.

## 8.5 어텐션 응용
### 8.5.1 구글 신경망 기계 번역 GNMT Google neural machine Translation
### 8.5.2 트랜스포머
RNN을 없애는 연구! self-attention 하나의 시계열 데이터를 대상으로, 하나의 시계열 내에서 각 원소가 어떻게 관련되는지 살펴본다.

계산량을 줄이고 GPU를 이용한 병렬 계산의 혜택을 누릴 수 있다. 학습 시간을 큰 폭으로 줄일 수 있고 품질도 좋다.

### 8.5.3 뉴럴 튜링 머신
외부 메모리를 통한 확장. 외부 메모리를 읽고 쓰면서 시계열 데이터를 처리한다.

## 8.6 정리
- 어텐션을 갖춘 모델은 인간처럼 필요한 정보에 주목할 수 있다.
- 번역, 음성인식 등 한 시계열 데이터를 다른 시계열 데이터로 변환하는 작업에서는 시계열 사이의 대응 관계가 존재하는 경우가 많다.
- 어텐션은 두 시계열 데이터 사이의 대응 관계를 데이터로부터 학습한다.
- 어텐션에서는 (하나의 방법으로서)벡터의 내적을 사용해 벡터 사이의 유사도를 구하고, 그 유사도를 이용한 가중합 벡터가 어텐션의 출력이 된다.
- 어텐션에서 사용하는 연산은 미분가능하기때문에 오차역전파법으로 학습할 수 있다.
- 어텐션이 산출하는 가중치를 시각화하면 입출력의 대응 관계를 볼 수 있다.
- 외부메모리를 활용한 신경망 확장 연구 예에서는 메모리를 읽고 쓰는데 어텐션을 활용했다.