# Chapter7 RNN을 사용한 문장 생성
이번 장에서는 언어 모델을 사용해 '문장 생성'을 해보고, seq2seq라는 새로운 구조의 신경망도 다뤄보자.

## 7.1 언어 모델을 사용한 문장 생성
I라는 단어를 줬을 때 다음 단어를 생성하는 방법?
1. 확률이 가장 높은 단어를 선택한다. deterministic. (결과가 일정하게 정해지는 '결정적'인 방법)
2. 확률적으로 선택한다. 각 후보 단어의 확률에 맞게 선택하는 방법. probabilistic. 이 방식에서는 샘플링 단어가 매번 다를 수 있다.

rnnlm_gen.py
generate_text.py

학습을 통해 가중치 매개변수를 갱신해나가고, 갱신된 가중치 매개변수를 이용함으로써 더 나은 결과를 낼 수 있다.

## 7.2 seq2seq
시계열 데이터를 다른 시계열 데이터로 변환하는 모델을 생각해보자.

시계열 데이터를 변환하는 기법 : 2개의 RNN을 이용하는 seq2seq sequence to sequence 방법이 있다.

seq2seq = Encoder-Decoder model
- Encoder: 입력 데이터를 인코딩하고(예: A-> 1000001) / 이 때 문장을 고정 길이 벡터로 인코딩한다.
- Decoder: 출력 데이터를 디코딩함(예: 100001 -> A)

![인코더가 하는 일](fig%207-7.png)

![seq2seq의 전체 계층 구성](fig%207-9.png)

순전파때에는 인코더에서 인코딩된 정보가 LSTM 계층의 은닉 상태를 통해 디코더에 전달되고, 역전파때에는 이 가교를 통해 기울기가 디코더로부터 인코더로 전해진다.

* toy problem : 머신러닝을 평가하고자 만든 간단한 문제

### 7.2.3 가변 길이 시계열 데이터
word2vec 등에서는 문장을 '단어'단위로 분할애왔는데, 문장을 반드시 단어로 분할해야하는건 아니다.

예: 덧셈 등. 57+5, 623+521 등 문제마다 문자 수가 다르다.

이런 덧셈 문제에서는 샘플마다 데이터의 시간 방향 크기가 다르기 때문에 학습에 필요한 '미니배치 처리'시 노력이 좀 더 필요하겠다.

가변 길이 시계열 데이터를 미니배치로 학습하기 위한 가장 단순한 방법 : 패딩을 사용하여 데이터 크기를 통일시킨다.

패딩은 원래는 없던 데이터인데 처리를 추가한것이므로, 정확도가 중요하다면 seq2seq에 패딩 전용 처리를 해야한다. 예: 디코더에 입력된 데이터가 패딩이라면 손실의 결과에 반영하지 않도록 한다.(소프트맥스 with loss 계층에 '마스크'기능 추가)<br/>
인코더에 입력된 데이터가 패딩이라면 LSTM계층이 이전 시각의 입력을 그대로 출력하게 한다.

* 훈련용/검증용/테스트용 데이터넷 복습. 훈련용으로는 학습을, 검증용으로는 하이퍼파라미터 튜닝을, 테스트용으로는 성능 평가를!

## 7.3 seq2seq 구현
seq2seq.py

* seq2seq의 stateful 구현? 긴 시계열 데이터를 처리하기 위해 상태를 오래 기억하려면 은닉 상태를 유지해줘야한다. 그렇지 않고 짧은 시계열 데이터 여러개를 처리하는것이면 문제마다 LSTM의 은닉 상태를 영벡터로 설정하면 된다.

* 확률적 선택 vs 결정적 선택? 문제에 따라 선택하자. 문장에는 답이 없으므로 확률적으로, 덧셈에는 답이 있는것이므로 결정적으로!

### 7.3.4 seq2seq 평가
[학습의 과정]
1. 학습 데이터에서 미니배치를 선택하고,
2. 미니배치로부터 기울기를 계산하고,
3. 기울기를 사용하여 매개변수를 갱신한다. 

[평가방법]
덧셈에 대해서는 정답(정답set)이 있으므로 답이 옳은지 아닌지의 정답률을 평가 척도로 쓰면 된다.

## 7.4 seq2seq의 속도 개선
- 입력 데이터 반전 기법 Reverse
입력 데이터를 반전시키면 많은 경우 학습 진행이 빨라져서 결과적으로 최종 정확도가 좋아진다고 함.
직관적으로는 기울기 전파가 원활해지기때문인것으로 보임.

- 엿보기 Peeky
인코더를 통해 변환된 고정길이 벡터 h를 좀 더 활용해보자. Affine계층과 LSTM계층에 h를 전달해준다.
![Peeky](fig%207-26.png)
중요한 정보를 여러 계층과 공유하여 더 올바른 결정을 내릴 가능성을 높인다.


## 7.5 seq2seq를 이용하는 애플리케이션
- 기계 번역 : 한 언어의 문장을 다른 언어의 문장으로 번역
- 자동 요약 : 긴 문장을 짧게 요약된 문장으로 변환
- 질의응답 : 질문을 응답으로 변환
- 메일 자동 응답 : 받은 메일의 문장을 답변 글로 변환

- 챗봇, 알고리즘 학습, 이미지 캡셔닝 등

## 7.6 정리
- RNN 을 이용한 언어 모델은 새로운 문장을 생성할 수 있다.
- 문장을 생성할 때는 하나의 단어를 주고 모델의 출력(확률분포)에서 샘플링하는 과정을 반복한다.
- RNN을 2개 조합함으로써 시계열 데이터를 다른 시계열 데이터로 변환할 수 있다.
- seq2seq는 Encoder가 출발어 입력문을 인코딩하고, 인코딩된 정보를 디코더가 받아 디코딩하여 도착어 출력문을 얻는다.
- 입력문을 반전시키는 기법, 또는 인코딩된 정보를 여러 계층에 전달하는 기법은 seq2seq의 정확도 향상에 효과적이다.
- 기계 번역, 챗봇, 이미지 캡셔닝 등 다양한 애플리케이션에 이용할 수 있다.
