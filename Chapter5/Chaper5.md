# Chapter5. RNN Recurrent neural network 순환 신경망

feed forward 신경망 : 흐름이 단방향인 신경망. 단순하여 이해하기 쉽고 많은 응용 가능. 그러나 시계열 데이터를 잘 다루지 못하는 단점 있음

=> RNN의 등장

## 5.1 확률과 언어 모델
### 5.1.1 word2vec을 확률 관점에서 보기
- CBOW 모델에서는 맥락이 주어졌을 때 타깃이 wi가 될 확률을 모델링한다.
- CBOW 모델에서 맥락의 윈도우 크기는 하이퍼파라미터가 된다. (이번 예에서는 왼2, 오0으로 설정)
- CBOW 모델이 다루는 손실함수는 아래 수식과 같다. (교차 엔트로피 오차에 의해 유도함 )![CBOW 손실함수](e%205-3.png)
- CBOW 모델의 학습으로 수행하는 일은 위의 손실함수를 최소화하는 가중치 매개변수를 찾는 것

- CBOW모델을 학습시키는 본 목적 : 맥락으로부터 타깃을 정확하게 추출하자.
- 부산물 : 단어의 의미가 인코딩된 '단어의 분산 표현'을 얻을 수 있다.

### 5.1.2 Language Model 언어 모델
언어 모델은 단어 나열에 확률을 부여한다. 이 시퀀스가 일어날 가능성이 어느 정도인지? 출력

확률의 곱셈정리로부터 최종으로 얻고자 하는 확률을 도출하면 아래와 같다. (언어모델이 다루는 사후 확률)
![언어 모델이 다루는 사후 확률](fig%205-3.png)
이것을 '조건부 언어 모델 conditional language model'이라고 한다.

* 마르코프 연쇄(모델) markov chain(model) : 미래의 상태가 현재 상태에만 의존해 결정되는 것.
* N층 마르코프 연쇄 : 직전 N개의 사건에만 의존할 때

## 5.2 RNN이란
Recurrent neural network = 순환하는 신경망

### 5.2.1 순환하는 신경망
기본적으로 순환을 하려면 닫힌 경로 또는 순환하는 경로가 존재해야한다.

순환하기 때문에 과거의 정보를 기억하는 동시에 최신 데이터로 갱신할 수 있는 것.

### 5.2.2 RNN의 순환 구조 펼치기
그림 5-8 참고
![순환 구조를 펼쳐 살펴보자](fig%205-8.png)

해당 층에서의 출력이 다시 입력이 되는데, 이 출력을 '은닉 상태 hidden state'또는 '은닉 상태 벡터 hidden state vector' 라고 한다.

### 5.2.3 BPTT 시간 방향으로 펼친 신경망의 오차역전파법
Backpropagation Through Time

그런데 긴 시계열 데이터를 학습할 때에는 그 크기에 비례하여 컴퓨팅 자원도 그만큼 필요하다.

시간 크기가 커지면 역전파 시 기울기가 불안정해지는 문제 해결이 필요하다.

### 5.2.4 Truncated BPTT
위의 문제를 해결하기 위해 신경망 연결을 적당한 길이로 끊는다.

작은 신경망 여러개로 만들어 오차역전파법 수행

순전파의 연결은 유지하고, 역전파의 연결만 끊어야한다.

** 순전파의 연결을 유지하므로 데이터를 순서대로 입력해야한다. **

### 5.2.5 truncated BPTT의 미니배치 학습
그림 5-15
![truncated BPTT의 미니배치 학습](fig%205-15.png)

## 5.3 RNN 구현
rnn.py

Time RNN : 시계열 데이터를 한꺼번에 처리할때 요렇게 표시하겠음. 이 책에서만!

## 5.4 시계열 데이터 처리 계층 구현
rnn.py

## 5.5 RNNLM 학습과 평가
### 5.5.2 언어모델의 평가
언어모델 : 주어진 과거 단어로부터 다음에 출현할 단어의 확률분포를 출력

성능 평가 척도로 퍼플렉서티 perplexity 혼란도를 자주 이용(확률의 역수, 작을수록 좋다.)

데이터가 1개일 때 설명한 '분기 수'를 데이터가 N개잉ㄴ 경우에서 평균한 것. '기하평균 분기 수'

![perplexity](e%205-12.png)

![perplexity](e%205-13.png)

### ~5.5.4 Trainer
train.py 참조

## 5.6 정리
- RNN은 데이터를 순환시킴으로써 시간순으로 데이터를 계속 흘려보낸다.
- RNN 계층 내부에는 은닉상태를 기억하는 능력이 추가된다
- RNN을 이용한 언어 모델은 단어 시퀀스에 확률을 부여하며, 조건부 언어 모델은 지금까지의 단어 시퀀스로부터 다음 단어의 확률을 계산해줌
- RNN의 순환 경로를 펼쳐 다수의 RNN 계층이 연결된 신경망으로 해석할 수 있고, 보통의 오차역전파법으로 학습 가능하다.
- 시계열이 너무 긴 경우 잘라서 수행한다. truncated
- truncated BPTT의 경우 역전파의 연결만 끊는것이고, 데이터는 순차입력해야한다.
- 언어 모델은 단어 시퀀스를 확률로 해석한다.
- 조건부 언어 모델은 그때까지 등장한 모든 단어의 정보를 기억할 수 있다.









