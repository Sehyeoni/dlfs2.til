# Chpater2 자연어와 단어의 분산 표현

- 자연어처리의 본질적 문제 : 컴퓨터가 우리의 말을 이해하게 만드는 것

## 2.1 자연어 처리란?
- natural language : 평소에 사람이 쓰는 말
- NLP(natural language processing) : 우리의 말을 컴퓨터에게 이해시키기 위한 기술(분야)

현재의 자연어처리 기술 예 : 검색엔진, 기계번역, 질의응답시스템, IME, 문장 자동요약, 감정분석 등

### 2.1.1 단어의 의미
단어의 의미를 잘 파악하는 표현 방법
- 시소러스를 활용한 기법 thesaurus(유의어 사전)
- 통계 기반 기법 
- 추론 기반 기법 : 신경망 활용 word2vec

## 2.2 Thesaurus 시소러스
뜻이 같은 단어나, 뜻이 비슷한 단어를 한 그룹으로 분류하고 단어 사이의 '상위와 하위' 혹은 '전체와 부분' 등 단어간 관계를 정의하기도 함

### 2.2.1 WordNet
자연어 처리 분야에서 가장 유명한 시소러스.

### 2.2.2 시소러스의 문제점
시소러스를 활용하면 단어의 의미를 컴퓨터에 전달할 수 있지만 "사람이 수작업으로 레이블링"하는 방식이라 큰 결점 존재
1) 시대 변화에 대응하기 어렵다.
2) 사람을 쓰는 비용이 크다.
3) 단어의 미묘한 차이를 표현할 수 없다.

=> 통계 기반 기법과 추론 기반 기법 등장. 단어의 의미를 다종으로 추출하여 수작업 제거

## 2.3 통계 기반 기법
말뭉치 corpus : 자연어 처리 연구와 애플리케이션을 염두에 두고 수집된 대량의 텍스트 데이터.

텍스트 데이터에 대한 추가 정보가 포함되는 경우가 있고(예: 품사), 그런 경우 말뭉치는 트리 구조 등 컴퓨터가 다루기 쉬운 형태로 가공되어 주어지는것이 일반적이다.

### 2.3.1 파이썬으로 말뭉치 전처리하기
corpus.py

### 2.3.2 단어의 분산표현
분산 표현 distributional representation : 단어의 의미를 정확하게 파악할 수 있는 벡터 표현

단어의 분산 표현은 단어를 고정 길의의 밀집벡터 dense vector로 표현한다.
* 밀집벡터 : 대부분의 원소가 0이 아닌 실수인 벡터

### 2.3.3 분포 가설
분포 가설 distributional hypothesis : 단어의 의미는 주변 단어에 의해 형성된다. 맥락 context가 의미를 형성한다.
- 맥락 context : 특정 단어를 중심에 둔 그 주변 단어
- window size : 맥락의 크기(주변 단어를 몇개나 포함할지)

### 2.3.4 동시발생 행렬 // 통계기반 기법
- 통계기반 statistical based 기법 : 어떤 단어에 주목했을 때, 그 주변 단어가 몇번이나 등장하는지 세어 집계하는 방법.
- 동시발생행렬 co-occurrence matrix : 모든 단어 각각의 맥락에 해당하는 빈도를 세어 표로 정리하면 행렬의 형태가 된다.
matrix.py

### 2.3.5 벡터간 유사도
벡터 사이의 유사도를 측정하는 대표적인 방법 : 벡터의 내적, 유클리드 거리, 코사인 유사도 등

코사인 유사도를 자주 이용한다.
- 코사인 유사도 : 두 벡터가 가리키는 방향이 얼마나 비슷한가? 같으면 1, 반대라면 -1
![코사인 유사도](e%202-1.png)

### 2.3.6 유사 단어의 랭킹 표시
most_similar.py

## 2.4 통계 기반 기법 개선하기
### 2.4.1 상호정보량
앞 절의서의 동시발생 행렬의 원소는 '발생횟수'인데, 그리 좋은 특징은 아니다. 단순하게 고빈도라고 해서 관련성을 많이 갖는것은 아니기 때문(예: the, a, etc...)

점별 상호정보량 pointwise mutual information(PMI) : 동시에 일어날 확률을 각각 일어날 확률로 나눈것
![PMI](e%202-3.png)
동시에 일어날 확률, 각각 일어날 확률은 동시발생 행렬으로부터 구한다.

* PMI의 문제점 : 두 단어의 동시발생횟수가 0이라면 -무한대가 되어버린다. => 구현시 양의 상호정보량 사용 positive PMI (PPMI)

** 구현시 음의 무한대로 빠지는 사태, 0이 되는 사태, 무한대가 되는 사태 예방이 중요한듯!

* PPMI의 문제점 : 어휘 수가 증가하면 벡터 차원 수도 증가한다.(비현실적 증가), 벡터 원소 대부분이 중요하지 않은 원소일 수 있다. 벡터는 노이즈에 약하고 견고하지 못하다. => 벡터의 자원 감소 기법 등장

### 2.4.2 벡터의 차원 감소 dimentionality reduction
중요한 정보는 최대한 유지하면서 차원을 줄여보자.

원소 대부분이 0인 희소행렬(또는 희소벡터)를 밀짋벡터로 변환하자.

특잇값분해 singular value decomposition (SVD) :
- 임의의 행렬을 세 행렬의 곱으로 분해(직교행렬, 대각행렬)
- 특잇값의 대각행렬. 특잇값 : 데이터를 넓게 분포시키는 축, 해당 축의 중요도
![SVD1](fig%202-9.png)
![SVD2](fig%202-10.png)

### 2.4.4 PTB 데이터셋
펜 트리뱅크 penn treebank 말뭉치의 한 종류. 주어진 기법의 품질을 측정하는 벤치마크로 자주 이용된다.

## 2.5 정리
- 단어를 이해시키기 위한 기법(시소러스, 통계기반, 추론(다음장))
- 시소러스 기반 기법의 의의와 한계
- 통계기반기법의 장점과 한계, 벡터 차원 감소, 희소벡터를 밀집벡터로.
- 말뭉치의 데이터를 다르기 쉽게 해주는 전처리 과정 구현 실습