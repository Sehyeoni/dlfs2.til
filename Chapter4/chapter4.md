# Chapter4 word2vec 속도 개선

이번 장의 목표 : 앞장에서의 CBOW 모델 성능 개선
- Embedding이라는 새로운 계층 도입
- 네거티브 샘플링이라는 새로운 손실 함수 도입

## 4.1 word2vec 개선 (1)
앞 장에서의 계산 성능 병목 2군데
- 입력층의 원핫 표현과 가중치 행렬의 곱 계산 (embedding 계층 도입으로 해결)
- 은닉층과 가중치 행렬의 곱 및 소프트맥스 계층의 계산 (네거티브 샘플링 손실함수 도입으로 해결)

### 4.1.1 Embedding 계층
MatMul 계층은 행렬의 특정 행을 추출하는 것이라 원핫 표현으로의 변환과 MatMul계층의 행렬곱 계산은 사실 필요가 없다.

=> 가중치 매개변수로부터 단어ID에 해당하는 행을 추출하는 계층을 만들자. = embedding 계층

### 4.1.2 Embedding 계층 구현
layer.py embedding class

## 4.2 word2vec 개선 (2)
### 4.2.1 은닉층 이후 계산의 문제점
은닉층 이후에서 계산이 오래 걸리는 곳 두군데
- 은닉층의 뉴런과 가중치 행렬의 곱
- 소프트맥스 계층의 계산

둘 다 어휘의 수가 많아질수록 계산량이 증가한다.

### 4.2.2 다중 분류에서 이진 분류로
- 다중 분류 multi-class classification 를 이진 분류 binary classification로 근사하자.

다중 분류 문제를 이진 분류 방식으로 해결해보자.

예) 맥락이 you와 goodbye일 때, 타깃 단어는 say입니까? 에  yes/no로 대답하는 신경망을 생각해내자

타깃 단어만의 점수를 구하는것!

### 4.2.3 시그모이드 함수와 교차 엔트로피 오차
note) 다중 분류의 경우 출력층에서는 소프트맥스함수를, 손실함수로는 교차엔트로피오차를 이용한다.<br/>이진 분류의 경우에는 출력층에서 시그모이드함수를, 손실함수로는 교차엔트로피오차를 이용한다.

### 4.2.4 구현
negative_sampling_layer.py

### 4.2.5 네티브 샘플링
다중 분류 문제를 이진 분류로 변환하기는 했지만, 부정적인 에를 입력하면 어떤 결과가 나올지?

긍정적 예에 대해서는 시그모이드 계층의 출력을 1에 가깝게, 부정적 예에 대해서는 0에 가깝게 해야한다.

모든 부정적 예를 대상으로 하여 이진 분류를 학습시킨다? => 그렇다면 늘어난 어휘수에 감당하기 위해 고안한 의미가 없어짐. 몇개만 샘플링 하자. => 네거티브 샘플링

긍정적 예를 타깃으로 한 경우 손실을 구하고, 부정적 예를 샘플링하여 손실을 구한다. 그리고 각각의 데이터의 손실을 더한 값을 최종 손실로 한다.

### 4.2.6 네거티브 샘플링의 샘플링 기법
샘플링도 잘 해보자. 말뭉치에서 자주 등장하는 단어를 많이 추출하고, 드물게 등장하는 단어를 적게 추출해서 확률분포대로 샘플링.

cf) unigram, bigram, trigram, ... 

### 4.2.7 네거티브 샘플링 구현
negative_sampling_layer.py

### 4.3 개선판 word2vec 학습
cbow.py

## 4.4 word2vec 남은 주제
### 4.4.1 word2vec 사용한 애플리케이션의 예
- word2vec으로 얻은 단어의 분산 표현은 비슷한 단어를 찾는 용도로 이용할 수 있다.
- 전이 학습 transfer learning이 가능하다. (해놨으니 가져다가 쓴다.)
- 단어, 문장 고정 길이 벡터로 변환할 수 있다. (bag-of_words, RNN 등)을 이걸 머신러닝에 적용할 수 있다.
- 다양한 자연어 처리 문제에서 word2vec으로 구축한 단어의 분산 표현을 이용해 정확도를 높일 수 있다.

### 4.4.2 단어 벡터 평가 방법
자주 사용되는 평가 척도
- 단어의 유사성을 활용한 평가 : 사람이 작성한 단어 유사도를 검증 세트를 사용해 평가한다. 코사인 유사도 점수를 비교해 사람 vs 기계 상관성 평가
- 유추 문제를 활용한 평가 : "king:queen=man:?"과 같은 유추 문제를 출제하고 그 정답률로 우수성을 측정

유추 문제의 결과로부터는 다음 사항을 알 수 있다.
- 모델에 따라 정확도가 다르다(말뭉치에 따라 적합한 모델 선택)
- 일반적으로 말뭉치가 클수록 결과가 좋다(항상 데이터가 많은게 좋다)
- 단어 벡터 차원 수는 적당한 크기가 좋다(너무 커도 정확도가 나빠짐)
 
유추 문제를 이용하면 단어의 의미나 문법적인 문제를 제대로 이해하고 있는지를 측정할 수 있다.
그러나 이 우수함이 애플리케이션에 얼마나 기여하는지와는 별도이다!

## 4.5 정리
- embedding 계층은 단어의 분산 표현을 담고 있으며, 순전파 시 지정한 단어ID의 벡터를 추출한다.
- word2vec은 어휘 수의 증가에 비례하여 계산량도 증가하므로, 근사치로 계산하는 빠른 기법을 사용하면 좋다.
- 네거티브 샘플링은 부정적 예를 몇개 샘플링하는 기법으로, 이를 이용하면 다중 분류를 이진 분류처럼 취급할 수 있다.
- word2vec으로 얻은 단어의 분산 표현에는 단어의 의미가 녹아들어 있으며, 비슷한 맥락에서 사용되는 단어는 단어 벡터 공간에서 가까이 위치한다.
- word2vec의 단어의 분산 표현을 이용하면 유추 문제를 벡터의 덧셈과 뺄셈으로 풀 수 있게 된다.
- word2vec은 전이 학습 측면에서 특히 중요하며, 그 단어의 분산 표현은 다양한 자연어 처리 작업에 이용할 수 있다.