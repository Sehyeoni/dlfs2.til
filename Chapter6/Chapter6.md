# Chapter6 게이트가 추가된 RNN

이번 장에서 살펴볼 내용 : '장기 기억'을 가능하게 하는 메커니즘, 구

앞장에서의 RNN은 시계열 데이터에서 시간적으로 멀리 떨어진 long term 장기의존관계를 잘 학습할 수 없어 성능이 좋지 못하다.

요즘은 앞장의 단순한 RNN대신, LSTM이나 GRU계층이 주로 쓰인다.<br/>
RNN이라 하면 LSTM을 지칭하는 경우가 흔하고, RNN을 기본RNN이라고 하기도 함

LSTM Long Short-Term Memory 이나 GRU에는 gate라는 구조가 더해져 있는데, 이것으로 시계열 데이터의 장기 의존 관계를 학습할 수 있다.현

## 6.1 RNN의 문제점
### 6.1.1, 6,1,2 기울기 소실 또는 기울기 폭발
정답이 주어졌을 때 이것을 학습할 때의 기울기 흐름 - 아래 그림 참고
![정답을 학습할 때의 기울기 흐름](fig%206-4.png)

RNN 계층이 과거 방향으로 '의미 있는 기울기'를 전달함으로써 시간 방향의 의존 관계를 학습한다.

이 기울기에는 학습해야 할 의미있는 정보가 들어있고, 그것이 과거로 전달되면서 장기 의존 관계를 학습할 수 있음.

그런데 이 기울기가 중간에 사그라들어 아무런 정보도 남지 않게 되면 가중치 매개변수 갱신이 끝난다. -> 장기 의존 관계 학습 불가

이전 장의 단순한 RNN에서는 시간을 거슬러 올라갈수록 기울기가 소실되거나, 기울기가 폭발되거나 대부분 둘 중 하나의 결과가 생긴다.

### 6.1.3 기울기 소실 vanishing gradients 과 기울기 폭발 exploding gradients 의 원인
역전파로 전해지는 기울기는 차례로 탄젠트, +, 행렬곱 연산을 통과한다.<br/>
tanh 노드를 지날때마다는 기울기가 계속 작아진다.
* 활성화 함수를 tanh 대신 ReLU를 사용하면 기울기 소실을 줄일 수 있다. 입력x가 0 이상이면 역전파 시 상류의 기울기를 그대로 하류에 흘려보내기 때문(기울기가 작아지지 않기 때문)

행렬곱을 지나면 가중치 크기가 1보다 크냐 작으냐에 따 기울기 크기가 지수적으로 증가하거나 감소한다.라

### 6.1.4 기울기 폭발 대책 : 기울기 클리핑 gradients clipping
![기울기 클리핑](e%206-0.png)

신경망에서 사용되는 모든 매개변수에 대한 기울기를 하나로 처리한다고 가정하고, g 로 표기<br/>
threshold를 문턱값으로 설정. g가 이 값을 초과하면 기울기를 수정

## 6.2 기울기 소실과 LSTM
### 6.2.1 LSTM의 인터페이스
LSTM 계층의 인터페이스에는 기억 셀 c 경로가 있다. memory cell
![RNN vs LSTM](fig%206-11.png)
기억 셀의 특징은 데이터를 자기 자신으로만(LSTM 계층 내에서만) 주고받는다는 것<br/>
은닉 층 h는 다른 계층으로 출력된다.

은닉 층 h는 기억 셀 c에 hanh 함수를 적용한다.

### 6.2.1 LSTM 계층 조립하기
LSTM에는 기억 셀 c가 있다. 과거로부터 시각 t까지에 필요한 모든 정보가 저장되어 있다고 가정한다.(혹은 그렇게 되도록 학습을 수행)

c를 바탕으로 외부 계층에 은닉 상태 ht를 출력한다.

Gate는 데이터의 흐름을 제어해서 다음 단계로 어느정도(열림 상태 openness) 데이터를 흘려보낼지 제어한다.<br/>
열림 상태를 제어하기 위해서는 전용 가중치 매개변수를 이용하고, 이 가중치 매개변수도 학습데이터로부터 갱신된다.<br/>
게이트의 열림 상태를 구할 때에는 시그모이드 함수를 사용하는데, 시그모이드 함수의 출력이 마침 0.0 ~ 1.0 사이의 실수이기 때문이다.

### 6.2.3 output gate, 6.2.4 forget gate, 6.2.5 새로운 기억 셀, 6.2.6 input gate
output gate : 다음 은닉 상태의 출력을 담당한다. (아래 o의 결과에 tanh(c)를 원소별 곱셈(hadamard product) 처리)
![e6-1](e%206-1.png) ![e6-2](e%206-2.png)

forget gate : 기억 중에서 불필요한 기억을 잊게 해주는 게이트
아래 계산을 수행한 다음, 이전 기억 셀인 c(t-1)을 아다마르 곱 하여 c(t)를 구한다.
![e6-3](e%206-3.png)

forget gate를 통해서 불필요한 기억을 삭제했으므로, 새로 기억할 정보를 기억 셀에 추가한다.<br/>
새로운 기억 셀을 이전 시각의 기억 셀에 더한다.
![e6-4](e%206-4.png)

input gate : 새로 추가되는 원소가 정보로써 가치가 얼마나 큰지 판단하여 가중치를 더한다.
![e6-5](e%206-5.png)

![fig6-18](fig%206-18.png)

### 6.2.7 LSTM의 기울기 흐름
기억 셀의 역전파에서는 +노드와 x노드를 지나게 된다.<br/>
+노드에서는 상류에서 전해지는 기울기를 그대로 흘리므로 변화가 일어나지 않고,<br/>
x노드에서는 아마다르 곱을 계산하여 매 시각 다른 게이트 값을 이용하게 된다.<br/>
따라서 동일한 가중치로 계산했을때에와 반면에 곱셈의 효과가 누적되지 않아 기울기 소실이 일어나기 어렵게 된다.<br/>
x노드에서의 계산은 forget 게이트가 제어하는데, 기억해야하는 정보에 대해서는 기울기가 소실되지 않고 전파되리라 기대할 수 있다.

## 6.3 LSTM의 구현
lstm.py

### 6.3.1 time LSTM 구현
lstm.py

## 6.4 LSTM을 사용한 언어 모델 구현
Rnnlm.py

학습 : train_rnnlm.py

## 6.5 RNNLM 추가 개선
### 6.5.1 LSTM 계층 다양화
RNNLM으로 정확한 모델을 만들고자 한다면, 많은 경우 LSTM 계층을 깊게 쌓아 효과를 볼 수 있다.<br/>
복잡한 패턴을 학습할 수 있게 된다.<br/>
몇 층이나 쌓아야할지?는 하이퍼파라미터이므로 처리할 문제의 복잡도나 준비된 학습 데이터의 양에 따라 적절하게 결정해야한다.

### 6.5.2 드롭아웃에 의학 과적합 억제
층을 다층화 하면 복잡한 의존 관계를 학습할 수 있을거라 기대할 수 있지만, 종종 overfitting을 일으킨다.<br/>

[해결방법?]
- 훈련 데이터의 양을 늘린다.
- 모델의 복잡도를 줄여준다.
- 모델의 복잡도에 페널티를 주는 정규화normalization도 효과적이다.
- dropout처럼 훈련시 계층 내의 뉴런 몇개를 무시하고 학습하게 한다. (일종의 정규화) 이 때, 시간축 말고 깊이 방향으로 삽입하자.

일반적인 드롭아웃은 시간 방향에는 적합하지 않은데, 최근 연구에서 시간 방향 정규화를 목표로 변형 드롭아웃을 제안함

Variational Dropout
![fig6-34](fig%206-34.png)
같은 계층에 속한 드롭아웃은 같은 mask를 공유하여 정보를 잃게 되는 방법을 고정시킨다. (= 정보가 지수적으로 손실대는 사태를 피한다.)

### 6.5.3 가중치 공유
weight tying
예) embedding 계층과 softmax앞단의 affine계층이 가중치를 공유한다.
![fig6-35](fig%206-35.png)
학습하는 매개변수 수가 크게 줄어들고, 정확도가 향상된다. 매개변수 수가 줄어든다함은 과적합이 억제되는 혜택으로 이어진다.

## 6.6 정리
- 단순한 RNN의 학습에서는 기울기 소실과 폭발이 문제가 된다.
- 기울기 폭발에는 기울기 클리핑, 기울기 소실에는 게이트가 추가된 RNN(LSTM, GRU ...)이 효과적이다.
- LSTM input gate, forget gate, output gate 총 3개의 게이트가 있다.
- 게이트에는 전용 가중치가 있으며, 시그모이드 함수를 사용하여 0.0~1.0 사이의 실수를 출력한다.
- 언어 모델 개선에는 LSTM 계층 다층화, 드롭아웃, 가중치 공유 등의 기법이 효과적이다.
- RNN의 정규화는 중요한 주제이며, 드롭아웃 기반의 다양한 기법이 제안되고 있다.