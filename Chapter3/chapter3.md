# Chapter3 word2vec
- 저번 장 주제 : 단어의 분산 표현 by 통계 기반 기법
- 이번 장 주제 : 단어의 분산 표현 by 추론 기반 기법. word2vec 구조를 차분히 들여다보고 이해하고 단순한 버전을 구현하기.
- 다음 장 주제 : 이번 장의 단순한 word2vec에 몇가지 개선을 더해 진짜 word2vec을 완성하기

## 3.1 추론 기반 기법과 신경망
### 3.1.1 통계 기반 기법의 문제점
- 통계 기반 기법 : 주변 단어의 빈도를 기초로 단어를 표현. 동시발생 행렬, SVD 적용
- 문제점 : 대규모 말뭉치를 다룰 때 문제가 발생한다. 현실적으로 규모에는 적합하지 않은 모델인것. O(n^3)의 비용 발생.

통계 기반 기법은 학습 데이터를 한꺼번에 처리한다(배치 학습)<br/>
신경망을 이용한 추론 기반 기법에서는 미니배치로 학습한다.<br/>

데이터를 작게 나눠 학습하므로 대규모 작업 소화 가능<br/>
여러 머신과 GPU를 이용한 병렬 계산이 가능하여 학습 속도를 높일수도 있음.<br/>
(다음 절에서 장점 재논의)

### 3.1.2 추론 기반 기법 개요
주변 단어 맥락을 활용해 빈 칸에 무엇이 들어갈지 추측한다. 이 문제를 반복해서 풀면서 단어의 출현 패턴을 학습한다.

모델은 맥락 정보를 입력받아 각 단어가 출현할 수 있는 확률을 출력하고, 올바른 추측을 내놓도록 학습시킨다.<br/>
학습의 결과로 "단어의 분산 표현(단어의 의미를 정확하게 파악할 수 있는 벡터 표현)"을 얻는것이 추론 기반 기법의 전체 그림

* 추론 기반 기법이나, 통계 기반 기법이나 모두 분포 가설에 근거하는 '단어의 동시발생 가능성'을 얼마나 잘 모델링하는가를 중요한 연구 주제로 삼고있음

### 3.1.3 신경망에서의 단어 처리
신경망에서는 단어를 '고정 길이의 벡터'로 변환해야한다.

대표적인 방법은 원핫 표현(또는 원핫 벡터)로 변환하는 것이다. (벡터 중 원소 하나만 1이고 나머지는 모두 0인 벡터)
![원핫표현, 입력층의 뉴런 표현](fig%203-5.png)

위 그림 3-5를 가중치를 포함한 완전연결계층에 의한 변환으로 나타내면 아래와 같음
![가중치를 포함한 완전연결계층에 의한 변환](fig%203-7.png)

* 편항을 이용하지 않는 완전연결계층은 행렬곱 계산에 해당함. 대부분 프레임워크에서는 편향을 이용할지 선택할 수 있게 해줌.

여기서 입력이 원핫표현이기때문에 행렬곱 수행시 결과는 w(가중치)의 행벡터 하나를 뽑아낸 것과 동일하다.

<< ? 이것이 무슨 의미가 있는 행위인지 잘 모르겠다. 가중치가 핵심인것같은데 아직 내용은 나오지 않았다. >><br/>
==> 가중치가 바로 분산 표현의 정체다. 어떻게 구하는지는 아직 안나왔다.함
==> 말뭉치덩이로 오차 줄여가는 방식으로 학습해서 구한다.

## 3.2 단순한 word2vec
CBOW continuous bag-of-words 모델을 신경망으로 구축해보자.<br/>
CBOW 모델은 활성화 함수를 사용하지 않는 간단한 구성의 신경망이다.<br/>
입력층이 여러개 있고, 그 입력등츨이 가중치를 공유한다.

### 3.2.1 CBOW 모델의 추론 처리
CBOW 모델은 맥락으로부터 타깃을 추측하는 용도의 신경망이다.

- 입력 : 맥락(단어들의 목록). 입력층의 갯수는 맥락으로 고려할 단어의 갯수와 동일하다.
- 출력 : 우리는 단어의 분산 표현을 원한다!
![CBOW 모델의 신경망 구조](fig%203-9.png)

가중치가 바로 단어의 분산 표현의 정체이다!

은닉층의 뉴런 수를 입력층의 뉴런 수보다 적게 하는것이 중요한 핵심이다. 이렇게 해야 단어 예측에 필요한 정보를 간결하게 담고, 밀집벡터 표현을 얻을 수 있다.

### 3.2.2 CBOW 모델의 학습
출력층에서 얻은 각 점수에 소프트맥스 함수를 적용하면 '확률'을 얻을 수 있다. 

CBOW 모델의 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정하는 일을 한다. 가중치에 단어출현 패턴을 파악한 벡터가 학습된다.의

이 모델은 다중 클래스 분류를 수행하므로, 소프트맥스(점수->확률)와 교차 엔트로피 오차(정답과의 오차를 손실로 학습해 사용)를 이용하면 된다.
![word2vec 계층 구성](fig%203-14.png)

### 3.2.3 word2vec의 가중치와 분산 표현
- W in : 각 단어의 분산 표현에 해당. 많은 연구에서 입력층 가중치만을 단어의 분산 표현으로 이용하고 있음.
- W out : 각 단어의 의미가 인코딩된 벡터가 저장되고 있다.

## 3.3 학습 데이터 준비
### 3.3.1 맥락과 타깃
말뭉치 corpus 로부터 맥락과 타깃을 만들어야한다. 학습하려면 자료가 있어야지.
1. 텍스트를 단어 ID로 변환해준다.
2. 맥락의 윈도우 크기를 정하고, 맥락과 타깃을 만들어낸다.
![맥락과 타깃 작성하기](fig%203-17.png)
3. 단어ID를 원핫표현으로 변환한다.

## 3.4 CBOW 모델 구현
simple_cbow.py 참조<br/>
가중치는 일단 랜덤으로 지정해놓고 시작한다.

1. 학습 데이터를 입력한다.
2. 기울기를 구한다.
3. 가중치 매개변수를 갱신해나간다.
4. 학습 후 가중치 매개변수를 취득한다(결과! 단어의 분산 표현)

## 3.5 word2vec 보충
### 3.5.1 CBOW 모델과 확률
- 동시확률 P(A,B) : 각 사건이 동시에 일어날 확률
- 사후확률 P(A|B) : B가 주어졌을 때 A가 일어날 확률

맥락을 통해 타깃을 구할 때의 문제는 "w1과 w3이 주어졌을 때 w2를 구하기"와 동일한 문제.<br/>
이것을 수식으로 표현해내면 아래와 같다.
![식3.1](e%203-1.png)

CBOW모델의 손실 함수는 아래와 같이 표현할 수 있다. 위의 확률에 로그를 취하고 (-)처리하면 된다. = 음의 로그 가능도(negative log likelihood)
![식3.2](e%203-2.png)

코퍼스 전체로 확장하면 아래 식으로 표현하면 된다.
![식3.3](e%203-3.png)

### 3.5.2 skip-gram 모델
skip-gram 모델은 CBOW에서 다루는 맥락과 타깃을 역전시킨 모델이다.

CBOW는 맥락으로부터 하나의 타깃을 추측하고 skip-gram은 타깃으로부터 주변의 여러 단어를 추측한다.<br/>
skip-gram이 좀 더 어려운 문제에 도전한다고 할 수 있고, 어 어려운 상황에서 단련하는 만큼 결과가 더 뛰어날 가능성이 커진다.

![skip-gram 모델이 신경망 구성 예](fig%203-24.png)

CBOW는 타깃 하나의 손실을 구하지만 skip-gram은 맥락의 수만큼 추측해야하기때문에 손실함수는 각 맥락의 손실의 총합으로 구성된다.

### 3.5.3 통계 기반 vs 추론 기반
- 통계 기반 :<br/>
말뭉치 전체 학습.<br/>
새 단어 생기는 경우 계산 처음부터 다시 해야함.<br/>
분산 표현에 단어의 유사성이 인코딩된다.

- 추론 기반 :<br/>
미니배치 학습.<br/>
새 단어 생겨도 지금까지 학습한것에 추가로 학습시키면 됨 -> 효율적<br/>
분산 표현에 단어의 유사성, 복잡한 단어 사이의 패턴도 파악되어 인코딩된다. 그렇다고 통계 기반보다 우위에 있는것은 아니다.

* skip-gram에서 다루는 네거티브 샘플링을 이용한 모델은 코퍼스 전체의 동시발생 행렬을 활용한다. 두 기법이 서로 특정 조건 하에서 연결되어있다고 할 수 있다. GloVe는 두 기법을 융합하여 사용했다.

## 3.6 정리
- 추론 기반 기법은 추측하는것이 목적이며, 그 부산물로 단어의 분산 표현을 얻는다.
- word2vec은 skip-gram모델과 CBOW모델을 제공한다.
- CBOW 모델은 맥락으로부터 타깃을 추측한다.
- skip-gram 모델은 타깃으로부터 맥락을 추측한다.
- word2vec은 가중치를 다시 학습할 수 있으므로, 단어의 분산 표현 갱신이나 새로운 단어 추가를 효율적으로 수행할 수 있다.
